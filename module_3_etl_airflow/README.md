# Module 3 â€“ ETL Pipelines & Workflow Orchestration

## ğŸ“Œ Module Overview
This module focuses on ***ETL processes and data pipeline orchestration***.
It demonstrates how transactional data is incrementally synchronized between databases and how automated workflows are scheduled and monitored using ***Apache Airflow***.

The module consists of ***two practical labs***:
1. Incremental ETL pipeline between databases
2. Workflow orchestration with Apache Airflow DAGs

---

## ğŸ¯ Learning Objectives
- Implement incremental ETL pipelines using Python
- Synchronize staging and production data warehouses
- Work with multiple database technologies (MySQL, IBM DB2, PostgreSQL)
- Build and schedule workflows using Apache Airflow
- Monitor and validate pipeline execution

---

## ğŸ“ Module Structure
```
module_3_etl_airflow/
â”œâ”€â”€ README.md                         â† Module documentation and explanation
â”‚
â”œâ”€â”€ lab_1_incremental_etl/            â† Incremental ETL: MySQL â†’ Production DWH
â”‚   â”œâ”€â”€ automation_db2.py             â† Incremental ETL script (MySQL â†’ IBM DB2)
â”‚   â”œâ”€â”€ automation_postgres.py        â† Incremental ETL script (MySQL â†’ PostgreSQL)
â”‚   â”‚
â”‚   â”œâ”€â”€ connectors/                   â† Database connection examples
â”‚   â”‚   â”œâ”€â”€ mysqlconnect.py           â† Python connector for MySQL (staging DB)
â”‚   â”‚   â”œâ”€â”€ db2connect.py             â† Python connector for IBM DB2
â”‚   â”‚   â””â”€â”€ postgresqlconnect.py      â† Python connector for PostgreSQL
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                         â† Source data and database setup files
â”‚   â”‚   â”œâ”€â”€ sales.sql                 â† SQL script to create and populate sales tables
â”‚   â”‚   â””â”€â”€ sales.csv                 â† Source CSV data for data loading
â”‚   â”‚
â”‚   â””â”€â”€ screenshots/                  â† Execution evidence and validation
â”‚       â”œâ”€â”€ get_last_rowid_db2.png    â† DB2: last loaded row identification
â”‚       â”œâ”€â”€ get_last_rowid_pg.png     â† PostgreSQL: last loaded row identification
â”‚       â”œâ”€â”€ get_latest_records.png    â† MySQL: incremental data extraction
â”‚       â”œâ”€â”€ insert_records_db2.png    â† DB2: incremental data insertion
â”‚       â”œâ”€â”€ insert_records_pg.png     â† PostgreSQL: incremental data insertion
â”‚       â”œâ”€â”€ synchronization_db2.png   â† DB2: successful ETL synchronization output
â”‚       â””â”€â”€ synchronization_pg.png    â† PostgreSQL: successful ETL synchronization output
â”‚
â””â”€â”€ lab_2_airflow_dag/                 â† Workflow orchestration with Apache Airflow
    â”œâ”€â”€ process_web_log.py             â† Airflow DAG definition
    â”‚
    â”œâ”€â”€ data/                          â† Input data for DAG execution
    â”‚   â””â”€â”€ accesslog.txt              â† Web server log file (ETL input)
    â”‚
    â””â”€â”€ screenshots/                   â† Airflow execution and monitoring evidence
        â”œâ”€â”€ dag_args.png               â† DAG default arguments definition
        â”œâ”€â”€ dag_definition.png         â† DAG configuration and schedule
        â”œâ”€â”€ extract_data.png           â† extract_data task implementation
        â”œâ”€â”€ transform_data.png         â† transform_data task implementation
        â”œâ”€â”€ load_data.png              â† load_data task implementation
        â”œâ”€â”€ pipeline.png               â† Task dependency pipeline (ETL flow)
        â”œâ”€â”€ submit_dag.png             â† DAG submission via Airflow CLI
        â”œâ”€â”€ unpause_dag.png            â† DAG unpause confirmation
        â””â”€â”€ dag_runs.png               â† DAG runs monitored in Airflow UI

```
---

## ğŸ›  Tools & Technologies
- Python â€“ ETL logic and automation
- MySQL â€“ Staging database
- IBM DB2 â€“ Production data warehouse (Option A)
- PostgreSQL â€“ Production data warehouse (Option B)
- Apache Airflow â€“ Workflow orchestration
- Bash â€“ Data extraction, transformation, and loading
- Docker / Cloud IDE (SN Labs) â€“ Execution environment

---

## Lab 1 â€“ Incremental ETL Pipeline (MySQL â†’ Production DWH)
### Description
This lab **implements an incremental ETL pipeline** that synchronizes data from a **staging database (MySQL)** into a production data warehouse.
The same ETL pattern is implemented and validated against ***two different production targets***:
***Option A***: IBM DB2
***Option B***: PostgreSQL
This demonstrates portability of ETL logic across different database technologies.

### ğŸ— Architecture
- ***Staging Database***: MySQL (`sales.sales_data`)
- ***Production Data Warehouse***: IBM DB2 or PostgreSQL
- ***Incremental Key***: `rowid`

### âš™ï¸ ETL Logic
1. Identify the last loaded record in the production data warehouse
(`SELECT COALESCE(MAX(rowid), 0)`)
2. Extract new records from the MySQL staging database
(`WHERE rowid > last_rowid`)
3. Insert incremental records into the production data warehouse

### ğŸ§© Implemented Functions

-`get_last_rowid()`
Retrieves the maximum rowid from the production data warehouse.
- `get_latest_records(last_rowid)`
Extracts new records from the MySQL staging database.
- `insert_records(records)`
Loads the incremental records into the production data warehouse.

### ğŸ§ª Validation & Evidence
Execution output confirms successful synchronization:
- Last row ID detected in production
- Number of new records extracted from staging
- Number of records successfully inserted into production
Screenshots in the `screenshots/` folder provide execution proof for:
- DB2-based ETL run
- PostgreSQL-based ETL run

---

## Lab 2 â€“ Data Pipelines with Apache Airflow
### ğŸ“– Description
This lab demonstrates workflow orchestration using **Apache Airflow** by building a DAG that processes web server log files.
The pipeline extracts data, applies transformations, and loads the processed output into an archive file. 
 
### ğŸ§  DAG Details
- ***DAG ID***: process_web_log
- ***Schedule***: Daily (@daily)
- ***Operator Type***: BashOperator
- ***Catchup***: Disabled

### ğŸ”„ Pipeline Tasks
1. **extract_data**
Extracts IP addresses from the web server log file.
2. **transform_data**
Filters out specific IP addresses from the extracted data.
3. **load_data**
Archives the transformed data into a tar file.

### ğŸ”— Task Dependency
```
extract_data â†’ transform_data â†’ load_data
```
### ğŸ“¦ Pipeline Artifacts
- `extracted_data.txt`
- `transformed_data.txt`
- `weblog.tar`

## ğŸ“Š Monitoring & Execution
- DAG submitted and triggered via Airflow CLI
- DAG unpaused and executed successfully
- Pipeline runs monitored using the Airflow Web UI
Execution screenshots are provided in the `screenshots/` directory.

---

## ğŸ¯ Skills Demonstrated
- Incremental ETL design and implementation
- Multi-database data synchronization
- Python-based data automation
- Workflow orchestration with Apache Airflow
- Bash-based data processing
- Monitoring and validation of data pipelines


## ğŸ” Security Note
All database credentials and sensitive connection details have been removed or masked for security reasons.
Environment variables are recommended for local execution.


## âœ… Module Outcome
- Incremental ETL pipelines successfully implemented and validated
- ETL logic tested on both IBM DB2 and PostgreSQL
- Automated workflows built and monitored using Apache Airflow
- Reliable and repeatable data integration achieved


